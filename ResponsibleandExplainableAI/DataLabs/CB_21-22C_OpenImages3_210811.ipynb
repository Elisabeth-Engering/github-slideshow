{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elisabeth-Engering/github-slideshow/blob/main/ResponsibleandExplainableAI/DataLabs/CB_21-22C_OpenImages3_210811.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUTCUP-I6dBv"
      },
      "source": [
        "# __Responsible AI: Open Images V4__\n",
        "\n",
        "For this exercise, you are going to work with the Open Images V4 data:\n",
        "\n",
        "> Open Images is a dataset of ~9M images that have been annotated with image-level labels and object bounding boxes. The training set of V4 contains 14.6M bounding boxes for 600 object classes on 1.74M images, making it the largest existing dataset with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). Moreover, the dataset is annotated with image-level labels spanning thousands of classes.\n",
        "\n",
        "We will perform an Exploratory Data Analysis (EDA) to identify (potential) instances of bias. Furthermore, you will learn to apply various fairness metrics, and debiasing techniques such as removing ('fairness through unawareness'), adding, and transforming images ('fairness through awareness') to the Open Images V4 dataset. \n",
        "\n",
        "__Documentation:__\n",
        "- [Open Images](https://storage.googleapis.com/openimages/web/index.html)\n",
        "- [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit)\n",
        "\n",
        "__Learning Objectives:__\n",
        "1. Identify, and describe (potential) instances of bias in the Open Images V4 dataset\n",
        "2. Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA0nkf2mqaHA"
      },
      "source": [
        "## __Learning Objective 1: Identify, and describe (potential) instances of bias in the Open Images V4 dataset__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwH5zNUJGSqd"
      },
      "source": [
        "\n",
        "### __Load the meta data__\n",
        "\n",
        "Before you start with the exercises, it is good practice to create a virtual environment because it will allow you to install packages and modify your Python environment without fear of breaking packages installed in other environments. For more information on how to create such an environment, see the Anaconda tutorial [Managing Environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html). \n",
        "\n",
        "__Step 1:__ Install, and subsequently import the necessary Python packages, set your working directory, and load the data files containing the class descriptions and labels.\n",
        "\n",
        "Data sources (via USB or cloud storage link, ask lecturer):\n",
        "- class-descriptions\n",
        "- train-annotations-human-imagelabels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJUXdLT_aTWo"
      },
      "outputs": [],
      "source": [
        "# Load necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns',100) # Makes sure all columns get displayed\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import io\n",
        "import skimage.io\n",
        "import imageio\n",
        "\n",
        "# Load data\n",
        "class_descriptions = pd.read_csv(r\"C:\\Users\\lisae\\Data\\OpenImages Human Train Labels\\class-descriptions.csv\"); # Reads in the csv file as a DataFrame\n",
        "train_annotations_human_imagelabels = pd.read_csv(r\"C:\\Users\\lisae\\Data\\OpenImages Human Train Labels\\train-annotations-human-imagelabels.csv\"); # Reads in the csv file as a DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdAD1O94adQ0"
      },
      "source": [
        "### __Merge the meta data__\n",
        "\n",
        "The table ```train-annotations-human-imagelabels``` contain information on the image ids, and class labels of the Open Images V4 dataset. The corresponding class descriptions can be found in the ```class-descriptions``` table. Merging the two tables will increase the interpretability of the data: As a human being, a class description (e.g. ```wedding```) is generally more informative than a class label (e.g.```'/m/081hv'```).\n",
        "   \n",
        "__Step 2:__ Merge the ```class-descriptions``` and ```train-annotations-human-imagelabels``` tables, and name it ``` train_label_description ```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHLIekdcdSYO",
        "outputId": "af69c56c-87c5-4fe2-fc78-02b24b5de894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns of class_descriptions.columns: Index(['LabelName', 'Description'], dtype='object')\n",
            " \n",
            "Columns of train_annotations_human_imagelabels: Index(['ImageID', 'Source', 'LabelName', 'Confidence'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print('Columns of class_descriptions.columns:', class_descriptions.columns) # Prints columns of the DataFrame\n",
        "print(' ') # Prints empty row\n",
        "print('Columns of train_annotations_human_imagelabels:', train_annotations_human_imagelabels.columns) # Prints columns of the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA0xerpw7xJf",
        "outputId": "65a1667e-455f-496c-ecd4-f9f957d2a18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            LabelName       Description           ImageID        Source  \\\n",
            "0         /m/0100nhbf  Sprenger's tulip  00017c3c4989432b  verification   \n",
            "1         /m/0100nhbf  Sprenger's tulip  00020d41f719ec78  verification   \n",
            "2         /m/0100nhbf  Sprenger's tulip  000492aa3eb327d4  verification   \n",
            "3         /m/0100nhbf  Sprenger's tulip  0004b368eabc26db  verification   \n",
            "4         /m/0100nhbf  Sprenger's tulip  0007c5e6448297b8  verification   \n",
            "...               ...               ...               ...           ...   \n",
            "27887034     /m/0zvk5            Helmet  ffef3e3e3ecec097  verification   \n",
            "27887035     /m/0zvk5            Helmet  fff7c7593ded46c6  verification   \n",
            "27887036     /m/0zvk5            Helmet  fff7fc0b862a971b  verification   \n",
            "27887037     /m/0zvk5            Helmet  fffaa9687bd7f700  verification   \n",
            "27887038     /m/0zvk5            Helmet  ffffda81903d6bb7  verification   \n",
            "\n",
            "          Confidence  \n",
            "0                0.0  \n",
            "1                0.0  \n",
            "2                0.0  \n",
            "3                0.0  \n",
            "4                0.0  \n",
            "...              ...  \n",
            "27887034         1.0  \n",
            "27887035         0.0  \n",
            "27887036         1.0  \n",
            "27887037         0.0  \n",
            "27887038         0.0  \n",
            "\n",
            "[27887039 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Merge the data\n",
        "train_label_description = pd.merge(class_descriptions, train_annotations_human_imagelabels, \n",
        "                                   how='left', on='LabelName') # Does a left merge on the column 'LabelName'\n",
        "\n",
        "# Print the DataFrame\n",
        "print(train_label_description) # Prints DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niJJt3l8zAQI"
      },
      "source": [
        "### __Exploratory Data Analysis (EDA) on the metadata, Part 1__\n",
        "\n",
        "__Step 3:__ Perform an Exploratory Data Analysis (EDA) on the table ``` train_label_description ```, and visualize your findings.<br>\n",
        "<br>\n",
        "Tip: Need inspiration for your EDA, check Kaggle's [Inclusive Images Challenge](https://www.kaggle.com/c/inclusive-images-challenge/data?select=train_human_labels.csv). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCpaq3PedSYQ",
        "outputId": "30cbb611-7b5f-4353-8bac-39c087f86eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27887039, 5)\n",
            "\n",
            "LabelName       object\n",
            "Description     object\n",
            "ImageID         object\n",
            "Source          object\n",
            "Confidence     float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Display Shape and Data types of the DataFrame\n",
        "print(train_label_description.shape) # Prints shape (dimensions) of the DataFrame\n",
        "print('') # Prints empty row\n",
        "print(train_label_description.dtypes) # Prints data types (classes) of the columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn10sVi3dSYR"
      },
      "source": [
        "By printing out the shape (dimensions) of the DataFrame, we can see that the dataset has 27.887.093 observations and 5 features and one of the features of the DataFrame is the target variable.<br>\n",
        "<br>\n",
        "By printing out the data types (classes) of the columns of the DataFrame, we can see that our dataset has a combination of categorical (object) and numeric (float) features. The most common data type (class) in the DataFrame is categorical (object)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvwnGaEidSYR",
        "outputId": "96956a56-4c69-4f10-991d-ed31e00d0280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     LabelName       Description           ImageID        Source  Confidence\n",
            "0  /m/0100nhbf  Sprenger's tulip  00017c3c4989432b  verification         0.0\n",
            "1  /m/0100nhbf  Sprenger's tulip  00020d41f719ec78  verification         0.0\n",
            "2  /m/0100nhbf  Sprenger's tulip  000492aa3eb327d4  verification         0.0\n",
            "3  /m/0100nhbf  Sprenger's tulip  0004b368eabc26db  verification         0.0\n",
            "4  /m/0100nhbf  Sprenger's tulip  0007c5e6448297b8  verification         0.0\n",
            "\n",
            "         LabelName Description           ImageID        Source  Confidence\n",
            "27887034  /m/0zvk5      Helmet  ffef3e3e3ecec097  verification         1.0\n",
            "27887035  /m/0zvk5      Helmet  fff7c7593ded46c6  verification         0.0\n",
            "27887036  /m/0zvk5      Helmet  fff7fc0b862a971b  verification         1.0\n",
            "27887037  /m/0zvk5      Helmet  fffaa9687bd7f700  verification         0.0\n",
            "27887038  /m/0zvk5      Helmet  ffffda81903d6bb7  verification         0.0\n",
            "\n",
            "           LabelName      Description           ImageID        Source  \\\n",
            "507179     /m/014vnq          Donburi  062ddf0ac67945a6  verification   \n",
            "8272760   /m/02q6v0_  Sugarcane juice  0577be9ddefa4e84  verification   \n",
            "8410204   /m/02r4717      Gift basket  d2a94d0b36103099  verification   \n",
            "15316185   /m/05r655             Girl  f8093a5a02cb33f8  verification   \n",
            "24310273    /m/0dv3j          Boiling  1ade36fd42a1f815  verification   \n",
            "\n",
            "          Confidence  \n",
            "507179           0.0  \n",
            "8272760          0.0  \n",
            "8410204          0.0  \n",
            "15316185         0.0  \n",
            "24310273         0.0  \n"
          ]
        }
      ],
      "source": [
        "# Display DataFrame\n",
        "print(train_label_description.head()) # Prints the first 5 rows\n",
        "print('') # Prints empty row\n",
        "print(train_label_description.tail()) # Prints the last 5 rows\n",
        "print('') # Prints empty row\n",
        "print(train_label_description.sample(5)) # Prints 5 random rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMEymyLydSYS"
      },
      "source": [
        "By printing out rows of the DataFrame I have gained new insights about the column names making sence with the data insight them and that there are no significant missing values (NaN) sighted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lnaEPPbdSYS",
        "outputId": "50997b35-f662-469a-ade6-0d490b8efe30"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Confidence\n",
            "count  2.788682e+07\n",
            "mean   4.820688e-01\n",
            "std    4.996784e-01\n",
            "min    0.000000e+00\n",
            "25%    0.000000e+00\n",
            "50%    0.000000e+00\n",
            "75%    1.000000e+00\n",
            "max    1.000000e+00\n"
          ]
        }
      ],
      "source": [
        "# Vizualize numeric feature\n",
        "train_label_description.hist( # Plots a histogram of the data’s numeric features in a grid\n",
        "    figsize=(5,5), # Increases the grid size\n",
        "    xrot=45) # Rotates the x-axis by 45 degrees\n",
        "plt.show() # Shows plot\n",
        "\n",
        "# Summary statistics of the numerical features\n",
        "print(train_label_description.describe()) # Prints summary statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXFZF8DSdSYS"
      },
      "source": [
        "By visualizing the numeric feature in the DataFrame I can see possible outliers that cannot be explained or might be measurement errors, numeric features that should be categorical and Boundaries that do not make sense. From the histogram, I noted that there are more 0.0 confidences then 1.0. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot6o8TktdSYS",
        "outputId": "db378267-d553-4922-ef6f-0ce60d15b660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LabelName Description           ImageID        Source\n",
            "count    27887039    27887025          27886817      27886817\n",
            "unique      19995       19698           5654987             2\n",
            "top     /m/01g317      Person  00006af3eaaad9b6  verification\n",
            "freq       906091      906091               544      22343544\n"
          ]
        }
      ],
      "source": [
        "# Summary statistics of the categorical features\n",
        "print(train_label_description.describe(include='object')) # Prints summary statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_O34fx9dSYT"
      },
      "source": [
        "By printing out the summary statistics of the categorical features, we get the count of the values of each feature, the number of unique classes, the top most frequent class, and how frequently that class occurs in the data set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGNyLBdQ2jIv"
      },
      "source": [
        "__Step 3:__ Subset the data by selecting one value (i.e. class) in the column ```LabelName``` located within the table  ```train_label_description```. Name the table ```subset```.\n",
        "\n",
        "__Examples:__\n",
        "\n",
        "LabelName|Description\n",
        ":-----:|:-----:\n",
        "'/m/03bt1vf'| woman\n",
        "'/m/01f43'| beauty\n",
        "'/m/019nj4'| smile\n",
        "'/m/0fczf'| nurse\n",
        "'/m/0fsbk0'| health care practitioner\n",
        "'/m/027qf2'| chemical engineer\n",
        "'/m/02fbcn'| bartender\n",
        "'/m/081hv'| wedding\n",
        "\n",
        "<br>You can also use other tools to filter through the ```class-descriptions``` table. For example, I personally like to use Microsoft Excel (do not tell Nitin or Bram, they will probably laugh at me...) to find interesting classes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CECk4JN88XGi",
        "outputId": "496a2eb1-41b6-43be-f77d-1004114fe78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        LabelName Description           ImageID        Source  Confidence\n",
            "426927  /m/014cnc     Student  00003e2837c7b728  verification         0.0\n",
            "426928  /m/014cnc     Student  0000599864fd15b3  verification         0.0\n",
            "426929  /m/014cnc     Student  000060e3121c7305  verification         0.0\n",
            "426930  /m/014cnc     Student  0000c7640c802faf  verification         0.0\n",
            "426931  /m/014cnc     Student  0000d6d1c44decb6  verification         0.0\n",
            "...           ...         ...               ...           ...         ...\n",
            "427748  /m/014cnc     Student  edd5b6e5d5d8cc6b  verification         1.0\n",
            "427749  /m/014cnc     Student  f019900f98a9ee8a  verification         1.0\n",
            "427750  /m/014cnc     Student  fb8d8f0bbdf3f5d0  verification         0.0\n",
            "427751  /m/014cnc     Student  ff4243eb2b686392  verification         1.0\n",
            "427752  /m/014cnc     Student  ff6966196a9940be  verification         0.0\n",
            "\n",
            "[826 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Subset data\n",
        "subset = train_label_description[\n",
        "    train_label_description['Description'] == 'Student'] # Selects of a specific column containing a value\n",
        "print(subset) # Prints DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OrEfWa39P_J"
      },
      "source": [
        "\n",
        "__Step 4:__ Merge the tables ```subset``` and```train_label_description``` to match the format:\n",
        "\n",
        "ImageID|Source_x|LabelName_x|Confidence_x|Description_x|LabelName_y|Confidence_y|Description_y\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "000002b66c9c498e|crowdsource-verification|/m/01kcnl|1|Birthday|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/010l12|0|Roller coaster|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/012c4n|0|Cucurbita|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/012mj|1|Alcoholic beverage|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/012yh1|1|Style|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/014j1m|0|Apple|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/014l8n|0|Yogurt|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/014sv8|1|Human eye|/m/081hv|0|Wedding\n",
        "000002b66c9c498e|verification|/m/01599|0|Beer|/m/081hv|0|Wedding\n",
        "\n",
        "<br>Name you newly merged table ```train_[fill in class description value]```. \n",
        "\n",
        "<br>The table should include a column filled with your chosen class label value (i.e. ```LabelName_y```), and another column containing the additional class labels (i.e. ```Description_x```) associated with a particular image (i.e. ```ImageID```). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEQJ5M82ikGb",
        "outputId": "8700138b-cfee-4f00-aa8b-a84250ac39ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      LabelName_x Description_x           ImageID      Source_x  Confidence_x  \\\n",
            "0       /m/014cnc       Student  00003e2837c7b728  verification           0.0   \n",
            "1       /m/014cnc       Student  00003e2837c7b728  verification           0.0   \n",
            "2       /m/014cnc       Student  00003e2837c7b728  verification           0.0   \n",
            "3       /m/014cnc       Student  00003e2837c7b728  verification           0.0   \n",
            "4       /m/014cnc       Student  00003e2837c7b728  verification           0.0   \n",
            "...           ...           ...               ...           ...           ...   \n",
            "36797   /m/014cnc       Student  ff4243eb2b686392  verification           1.0   \n",
            "36798   /m/014cnc       Student  ff4243eb2b686392  verification           1.0   \n",
            "36799   /m/014cnc       Student  ff4243eb2b686392  verification           1.0   \n",
            "36800   /m/014cnc       Student  ff4243eb2b686392  verification           1.0   \n",
            "36801   /m/014cnc       Student  ff6966196a9940be  verification           0.0   \n",
            "\n",
            "       LabelName_y  Description_y      Source_y  Confidence_y  \n",
            "0        /m/011_dp  Membranophone  verification           0.0  \n",
            "1        /m/011l78           Team  verification           0.0  \n",
            "2        /m/011_my      Idiophone  verification           0.0  \n",
            "3      /m/011q46kg      Container  verification           1.0  \n",
            "4        /m/012c4n      Cucurbita  verification           0.0  \n",
            "...            ...            ...           ...           ...  \n",
            "36797     /m/03q69     Human hair  verification           1.0  \n",
            "36798     /m/04rky         Mammal  verification           1.0  \n",
            "36799     /m/09j2d       Clothing  verification           1.0  \n",
            "36800     /m/0k0pj     Human nose  verification           1.0  \n",
            "36801    /m/014cnc        Student  verification           0.0  \n",
            "\n",
            "[36802 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "# Merge the data\n",
        "train_Student = pd.merge(subset, train_label_description, \n",
        "                                   how='inner', on='ImageID') # Does a inner merge on the column 'ImageID'\n",
        "\n",
        "# Print the DataFrame\n",
        "print(train_Student) # Prints DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfqG2qOC_jMM"
      },
      "source": [
        "As you may have noticed the data contains a column ```Confidence[_x/y]```, which includes positive as well as negative label values (1 and 0 confidence respectively. \n",
        "\n",
        "> ... Positive labels are classes that have been verified to be in the image while negative labels are classes that are verified to not be in the image. Negative labels are useful because they are generally specified for classes that you may expect to appear in a scene but do not. For example, if there is a group of people in outfits on a field, you may expect there to be a ball. If there isn’t one, that would be a good negative label ([Source](https://medium.com/voxel51/loading-open-images-v6-and-custom-datasets-with-fiftyone-18b5334851c3)).\n",
        "\n",
        "A negative label can be an indication that there is bias present in the dataset. When you take a look at the wedding example, the value 'Ivory' is common in the case of a negative ```Confidence_y``` value. In other words, when the human annotator encounters an image with a ```Description_x``` value of 'Ivory' he/she is likely to incorrectly label ```Description_y``` as 'wedding' (i.e. false positive). In a traditional western wedding, the bride often dresses in white/ivory etc. But, that does not mean that all brides adhere to this clothing tradition. Take a country like Marocco, where some brides wear a Lebsa lfasiya:\n",
        "\n",
        ">Lebsa lfasiya is the traditional dress from the Fez region, also called \"ebsa lekbira\" (the great outfit). Its large size and shape and accompanying elaborate jewelry characterize this type of wedding dress. It can be white, red, or green ([Source](https://www.moroccoworldnews.com/2020/07/310720/marrying-love-and-fashion-wedding-dresses-in-morocco)).\n",
        "\n",
        "<img src=\"https://www.moroccoworldnews.com/wp-content/uploads/2020/07/Marrying-Love-Fashion-Wedding-Dresses-and-Morocco.jpg\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "*Figure 1. A woman wearing a traditional Maroccan wedding dress called a Lebsa lfasiya.*\n",
        "\n",
        "Quite different from a traditional western wedding dress, right?\n",
        "\n",
        "__Step 5:__ Subset the data into two datasets: one where the value of ```Confidence_y```  equals 0 named ```[fill in class description value]_negative``` , and another where the value of ```Confidence_y``` equals 1 named ```[fill in class description value]_positive```. Count the most common ```Description_x``` values for both subsets. Visualize your results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCh4trNmEw9h",
        "outputId": "01653da6-fa7d-4150-ea1f-e3cbeb560c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student    3050\n",
            "Name: Description_x, dtype: int64\n",
            "\n",
            "Student    7303\n",
            "Name: Description_x, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Subset the data into two datasets\n",
        "Student_positive = train_Student.loc[( # Creates dataset were value equals 1\n",
        "    train_Student['Confidence_x'] == 1) & (\n",
        "    train_Student['Confidence_y'] == 1)] \n",
        "\n",
        "Student_negative = train_Student.loc[( # Creates dataset were value equals 0 \n",
        "    train_Student['Confidence_x'] == 0) & ( # Bias\n",
        "    train_Student['Confidence_y'] == 1)] # All other labels\n",
        "\n",
        "# Count the most common Description_x values for subsets\n",
        "print(Student_positive['Description_x'].value_counts()) # Prints count\n",
        "print('') # Prints empty row\n",
        "print(Student_negative['Description_x'].value_counts()) # Prints count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAdjK_D7dSYU",
        "outputId": "b1fa846f-0320-4c15-a857-6bf7c9de609d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student        280\n",
            "Smile           82\n",
            "Person          74\n",
            "Education       66\n",
            "Clothing        62\n",
            "              ... \n",
            "Fedora           1\n",
            "Cowboy hat       1\n",
            "Knit cap         1\n",
            "Trench coat      1\n",
            "Dress            1\n",
            "Name: Description_y, Length: 498, dtype: int64\n",
            "\n",
            "Person              145\n",
            "Smile               144\n",
            "Clothing            102\n",
            "Woman                87\n",
            "Man                  86\n",
            "                   ... \n",
            "Fête                  1\n",
            "Carnival              1\n",
            "Tribe                 1\n",
            "Costume hat           1\n",
            "Angel's trumpets      1\n",
            "Name: Description_y, Length: 1340, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Count the most common Description_y values for subsets\n",
        "print(Student_positive['Description_y'].value_counts()) # Prints count\n",
        "print('') # Prints empty row\n",
        "print(Student_negative['Description_y'].value_counts()) # Prints count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf_I7jTtEzLK"
      },
      "source": [
        "__Step 6:__ Did you find any instance of bias in the Open Images V4 dataset? If your answer is 'No', you need to dig deeper into the data! Elaborate on your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0liK1WjxdSYV"
      },
      "source": [
        "<font color='salmon'> To find instances of bias in the Open Images V4 dataset I will use three steps. This workflow (Discovering Biases in Image Datasets with the Crowd, n.d.) is further augmented by back-end text processing techniques to deal with the noisy inputs from the crowd. The preliminary results suggest that this workflow is promising in uncovering potential biases in visual datasets, that is why I chose to use it.<br>\n",
        "<br>\n",
        "1. Inspect random samples of images from the dataset and describe their similarity;\n",
        "2. Review separate random samples of images from the dataset and provide answers to questions solicited\n",
        "from the previous step;\n",
        "3. Judge whether statements of the image dataset that are automatically generated using the questions and answers collected accurately reflect the real world.</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CopK0WeXdSYV"
      },
      "source": [
        "<font color='salmon'> **Inspect random samples of images from the dataset and describe their similarity**<br>\n",
        "<br>\n",
        "I started with creating a code that will open up a tab with a random image from the \".\\\\Student\\\\\" folder on my computer that contains downloaded Images using OIDv4 Toolkit.</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9oNVI9edSYV"
      },
      "outputs": [],
      "source": [
        "# Load required packages\n",
        "import random\n",
        "import os\n",
        "\n",
        "path = \".\\\\Student\\\\\" # Stores path to folder\n",
        "files = os.listdir(path) # Returns list with all names of the images in folder\n",
        "d = random.choice(files) # Return a random image from the list\n",
        "os.startfile(path + d) # Opens new tab with random image of the folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5j2VT6dSYV"
      },
      "source": [
        "<font color='salmon'> By running the code five times in a row, it gives me five images chosen at random from the \".\\Student\\\" folder on my computer that contains downloaded Images using OIDv4 Toolkit.<br> \n",
        "<br>\n",
        "When you look at the five images chosen at random, you can see that they have some similarity:\n",
        "- Include multiple humans;\n",
        "- Include technology;\n",
        "- Mostly include humans with a light skintone and dark hair;\n",
        "- Humans sitting next or across from each other.</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fvdy8d5dSYV"
      },
      "source": [
        "![613a969c71f65be8.jpg](attachment:613a969c71f65be8.jpg)\n",
        "*Image 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViVQ13JvdSYV"
      },
      "source": [
        "![f12de81b2b2a7d79.jpg](attachment:f12de81b2b2a7d79.jpg)\n",
        "*Image 2*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTRkdwudSYV"
      },
      "source": [
        "![003185aacd1c0be9.jpg](attachment:003185aacd1c0be9.jpg)\n",
        "*Image 3*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giRKVcrEdSYW"
      },
      "source": [
        "![32ef8d7ac81981a6.jpg](attachment:32ef8d7ac81981a6.jpg)\n",
        "*Image 4*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJLCHYKYdSYW"
      },
      "source": [
        "![621989719561c18d.jpg](attachment:621989719561c18d.jpg)\n",
        "*Image 5*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjcJCqxCdSYW"
      },
      "source": [
        "<font color='salmon'> **Review separate random samples of images from the dataset and provide answers to questions solicited from the previous step**<br>\n",
        "<br>\n",
        "When we look at the similarities in the images, we can create the following questions:\n",
        "1. How many humans does the image include?\n",
        "2. What technology does the image include?\n",
        "3. How do the humans in the image look like?\n",
        "4. How are the humans sitting in the image?\n",
        "5. What does the image include besides humans and technology?<br>\n",
        "<br>  \n",
        "Image 1\n",
        "- The image includes two humans;\n",
        "- The image includes a monitor with a keyboard and a mouse pad;\n",
        "- One of the humans has a dark skintone and has short dark hair. The other human has a light skintone and long dark hair.\n",
        "- The humans sitting across from each other;\n",
        "- The images also includes a desk, chairs, papers and an office enviorment.<br>\n",
        "<br>\n",
        "Image 2\n",
        "- The image includes four humans;\n",
        "- The image includes a laptop;\n",
        "- All four humans have a light skintone and dark hair;\n",
        "- The humans sitting next and across from each other;\n",
        "- The images also includes a desk, papers and a pen.<br>\n",
        "<br>\n",
        "Image 3\n",
        "- The image includes three humans;\n",
        "- The image includes two laptops with a mouse pad;\n",
        "- All three humans have a light skintone and dark hair;\n",
        "- The humans sitting next and across from each other;\n",
        "- The images also includes a desk, papers and drinking cups.<br>\n",
        "<br>\n",
        "Image 4\n",
        "- The image includes two humans;\n",
        "- The image includes a laptops;\n",
        "- Both humans have a light skintone and dark hair;\n",
        "- The humans sitting next from each other;\n",
        "- The images also includes a chairs, papers and an office enviorment.<br>\n",
        "<br> \n",
        "Image 5\n",
        "- The image includes seven humans;\n",
        "- The image includes one tablet;\n",
        "- All seven humans have a light skintone and dark hair;\n",
        "- The humans sitting next from each other;\n",
        "- The images also includes a chairs.</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWEVS3opdSYW"
      },
      "source": [
        "<font color='salmon'> **Judge whether statements of the image dataset that are automatically generated using the questions and answers collected accurately reflect the real world**<br>\n",
        "<br>\n",
        "When we look at the inspections and reviews on the images, we can see clearly that four out of five images only include humans with a light skin tone and dark-colored hair. We can see this as skin tone bias. This can be caused by a data set with not a lot of images with humans with a dark skin tone labeled as 'Student'. Another possibility can be that the humans responsible for training this Machine Learning (ML) system made the program biased. Whether the Open Images dataset is a technological continuation of intentional prejudices or not, incomplete data is still the main cause of artificial intelligence (AI) biases.</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL6v1ykuOv0G"
      },
      "source": [
        "### __Load the image data__\n",
        "\n",
        "Downloading the Open Images V4 data is quite cumbersome. The (meta) data files are large, and often need a good amount of preprocessing to make them ready for an ML analysis. The OIDv4 ToolKit enables you to download a subset of the data (less time spend on downloading data means more time for analysis...yeah!). Conveniently, it automatically transforms the image (meta) data into the correct format for Keras.\n",
        "\n",
        "__Step 7:__ Clone the [OIDv4 ToolKit](https://github.com/EscVM/OIDv4_ToolKit) GitHub repository. \n",
        "\n",
        "__Step 8:__ Open the cloned repository in the Command Prompt (i.e. terminal). For more information, run the code to see the tutorial below: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYEMS8lRHeYd",
        "outputId": "7bf262ee-3040-4192-893f-dc3767fceaab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lisae\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bgSSJQolR0E\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bgSSJQolR0E\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmNsSRsaIqAE"
      },
      "source": [
        "__Step 9:__ Activate your virtual environment in the Command Prompt, and follow OIDv4 ToolKit's installation procedure.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM6Gu-iYdSYW"
      },
      "source": [
        "__Step 10:__ Follow the instructions listed under the heading '3.0 Download images from Image-Level Labels Dataset for Image Classifiction' of the OIDv4 ToolKit repository. Use the code from this section to download the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVyqrA0QPHrp"
      },
      "source": [
        "### __Exploratory Data Analysis (EDA) on the metadata, Part 2__\n",
        "\n",
        "Now we have loaded the image data, you want to redo some of the steps listed in part 1 of the EDA. But before we proceed with the EDA, you need to remove some rows in the ```train_[fill in class description value]``` table. If a value in ```ImageID``` cannot be linked to actual images in your directory, you need to remove the corresponding row. \n",
        "\n",
        "__Step 11:__ Create a list containing all the image names (i.e. ```ImageID```) without its extension (e.g. .jpg), and use it to filter out all the redundant rows in your ```train_[fill in class description value]``` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8p6Pwo8dSYX",
        "outputId": "f8d1988c-b6f1-4a01-e85a-66379f398d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: C:\\Users\\lisae\\Artificial Intelligence (AI)\n"
          ]
        }
      ],
      "source": [
        "# Load required package\n",
        "import os\n",
        "\n",
        "# Print current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd())) # Prints current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0zf4Nz4dSYX",
        "outputId": "15c1a48c-915a-4ba7-90be-b94ee5325e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['00003e2837c7b728', '00244fb73e839998', '002730ebd4e115d1', '002b526d760fcd22', '003185aacd1c0be9', '00495759b6bb49c3', '00b3e932597f2ece', '00c1b2e23be6d099', '00c4667f5015b911', '00dc7d6faa0b8ed6', '00e57d3e8d9b9a55', '011d79b4b8aefdd9', '016eead5a0887551', '019a1d9e8ab79a70', '025a6afb25592b5f', '0270055c079d6790', '02a8723315baa4e2', '02b85e8256223bb0', '02c174c069aceed8', '032a24cd420692e1', '037d3080c8e2a715', '03aea9956c16f5d1', '03b51991527dae7c', '03b7ec6b8fbbc069', '03c386d5fc59e6f9', '03e1f1b29c6c0636', '048c8e1570f3a421', '04a50eb01027f5ed', '04a8aea09c797a20', '04d4c122868fde50', '04f6d57ef840379c', '053f99b204c827a9', '0551af8b30559b3e', '05716769253ceab4', '058b43c67f38b380', '05bf84199dfc47ea', '05e0c1214e70cdd0', '06802da4cec247b3', '068c6a6381647e65', '06948169a9520de6', '06a867913fa1cf75', '074720f9ff439b9a', '079490f263caeae2', '080f5b39b949742e', '082aa064f67acf70', '095cc1a0cdc43459', '0999be3619d01043', '09b8776169fb44cd', '09ce3d43f5fddec4', '09fdec05a54afb2e', '0a1bb0b87f908491', '0a2232051a6b58ab', '0aa612dc968ae580', '0ac53e773591d460', '0accdab975c55548', '0b14e2f64f62356d', '0b9e81ed560895dc', '0ba9dfbe26db0255', '0be7bcbe3c03caa5', '0c0d7bdff0171175', '0c404e08af991368', '0ce7970658d31204', '0d22cc03755068a7', '0d2a29d48927c52c', '0d390dde672d1256', '0d3946ef55533f13', '0ddf2724de1e5ed0', '0e1855ef503d2d09', '0e396948dc9d1598', '0e583f6fa777986c', '0f356dc54d92c450', '0fa47b14fc5ac7ec', '0fc938f92aca76f9', '1055390c9e4d91f9', '107cdd1891f0a0f5', '109fb85b103122cc', '10c1ccb35096e79d', '10e0442e0364d66c', '118219f0db58998f', '119c4db647288995', '11a52440c651c829', '11abf753e20b64c4', '11b1c4d521cb78c3', '11ca2f689c8259e2', '12247149d1c7e77b', '1237acd4c13625a2', '12da01fa53fb8335', '12da508dc84a94ee', '1311dcc534e4e1cf', '1339fc5f123d8fa5', '136435f25573702e', '13643eaf4385663d', '137b07a5b73fc40b', '1387646901a8e4c0', '138d14bec69c6693', '138fee14effeede2', '13a1eca69537c565', '13cd465e4d0f0158', '13f157037d59b59c', '13f78892f7cc55fc', '1403e826cc4d6f58', '149a0b6e3213e9be', '14fbbb097fbc70e0', '1544fe1fbf86e6ab', '1613dbf484db91cc', '1674c7f2e0b1beea', '168e927b7ab753a6', '16b44fabb8836167', '16dac3c13c971889', '173b4d9253705572', '1747684920dd6439', '177b6c12e5624db8', '1784fea157c10e9d', '17c3472ab84f0ad5', '1825b9784a28139a', '18426843f0ca9871', '184b82ba6eaf9724', '1868e2f97085ed6a', '1882d72d9b670bdc', '1895dd60a8ad0b31', '18b96565ff07a0c6', '18de491d249b317c', '19171f7f74d545b3', '1924e3a2cccd3e54', '19257c2b83604423', '199e3de441972a23', '199f17c2194cfe63', '19b76946f54e0596', '19bf62513cb1a5f2', '1a03423fc1a6314c', '1a367a1772cd6b10', '1a5b516cc097b088', '1a5cf3875e1eccb7', '1b38496bbeceec30', '1b5efebc9e19bee2', '1b75b89072f81631', '1bb6620fedb9f294', '1bdb981ee3836283', '1c0b76f793de174e', '1c2100c8539ea65b', '1c55134ddc2aa624', '1c8f056bfa5c335e', '1d2724a47123812f', '1ddc944b23c6467f', '1e80ab15643700d1', '1ea489ff0df8c78b', '1ec2d0e15786b412', '1ecff7c2db5e857f', '1f4fa9327cd7d768', '1f567a46011697a4', '1fa5af7854a60c3e', '200d64451421648c', '202bf7ae7a53cdaf', '2056132ea9c88e61', '20bf19e3068dc186', '20ca0bc92c2f779e', '20dee1754c6d2e33', '2110214059ef07c8', '21952d1c4fdf0e3c', '219888cc71abfbad', '223dd9693d862860', '224186595d4f9236', '22c20922c7958cf1', '2358b33076d621e9', '23d05275fd7be8dd', '2400f7bb3ebc5ae8', '240b875c12c4b6b7', '244632e5912e3ab4', '245c15ff0aa2c02b', '24df05d8b119530a', '252d9115e7ee809c', '25444f2c2d36d9da', '25793f112f1f31b5', '25c36d171eef8ade', '2655c1dd8dd0d09a', '266a8b7b8f442d2c', '26f25e32fa678edc', '27007bba3e89001d', '273ba76ea7c7d5f5', '2758fdec69a51fb8', '277097c1ed46348a', '27abda5fcbc400d7', '27b18513ddbc54c7', '2829fc1e73d346b8', '2884d60de19270c2', '289e6d7a25063466', '28a27891ee2d1db1', '28c9a3393086c057', '28d6040acf21504d', '28ed9106f66a3634', '28fb2814bfa1cc24', '291444008e8dcae7', '291545569c568c65', '29405f4464f3eb48', '296d2ce4b8fffc36', '299f5199bb00fef1', '29a85fa466054c3d', '29c8c4d5e6ad6e77', '2a3e711c5da016ac', '2a56be5d27fda208', '2a87b9d261a3d435', '2ab9bd1a13e67a06', '2ac18c3299534c00', '2ad2c92bbb04ed5a', '2ad9bd5251e769d7', '2b5bf8c418d8e179', '2b6d3703a15ae2d6', '2b6ec1421fc50039', '2bcaf3eb83ea5eb9', '2bd70175f2cddf81', '2c35a8e408010a07', '2cbe13727e8fbf99', '2d3fce9597a794fa', '2d496a008ed45eaa', '2d5c712f175b6d92', '2d68206ab0af893e', '2d7709346a0c7dc5', '2d7fdc2a99ad1a38', '2d83e8a9313a33d1', '2e08902f702b8cbd', '2e574846e625abcd', '2e7b3cc03e534963', '2e93d2e0d52bb7bd', '2ea114f3d6b9d051', '2eb5b381f4365d94', '2eca3acb60688dc1', '2ed3c0718f4b78a2', '2ef0854fe92ddfc3', '2ef1170ac37ac0fb', '2f092fbf84f70bf6', '2f9a52f792cfa0c7', '311f2029376528b3', '31746dc6e8887e30', '3176f03e1a943e02', '31a6535ff79b1198', '31bc50bff99069e9', '31f508bfb1741631', '3226debc70c492d0', '32a8fb8ca61a3576', '32ef8d7ac81981a6', '32efc3cc2353ff19', '32f773557cff8c93', '3351b808626fc714', '33583050c3d53e00', '339c42154a4dd292', '33aa574d9d05b281', '33cc7e624f38ad8c', '340437941f4ecedd', '34183ca086feefc4', '343d194311c71673', '3480566c1e932a25', '34df53fb94cdc4b3', '35b8fe747552c78a', '361827862d9d96ba', '363f74d2d5c4d773', '36582e27d0e335de', '36ae97919e405172', '36c39d46a048d813', '370cae43576d0a28', '3726632619c0b664', '3780d8c892479381', '37b2593127a47b2c', '37f64c3a87f6b3e7', '3804cef9dc278c18', '382bafe15b7d3990', '383756ed8a5ee867', '384984c9308cac8c', '385737855fdca685', '38b0b63034e1f01f', '39148e6bd424539f', '39283086d7e5fdd4', '3929d01908cae00c', '3968ee0150d02d68', '397874a2169543cf', '3995313ea888552e', '39c4ef36ebda0cb0', '39d9d0f733437884', '39d9fa55ec727328', '39f826610b0cd5ef', '3a2117f888bd8bdd', '3a89aa16fd946c40', '3a8d34f7bc3ddb88', '3b3b8e9dea1283a2', '3b64cc25088639b8', '3bbd3474f7e2002b', '3be367e63c6281ab', '3be82b6d155ee45d', '3bfa8dec69614536', '3c2015726992f202', '3c777ec1833030ac', '3c7a91d003550d3e', '3c9aa5d1151e6946', '3ceec224fe6465f1', '3d70499b5994cfed', '3e442b8335edcfe1', '3f29e618023fbcf1', '3f6d39c6124630f8', '3faad93ac31cf19e', '3fad628a5258e439', '40043a6923e78c39', '4050615d61c133b2', '409fe082cbd72c01', '410b49b510e86ad7', '41256fe68d469394', '41b9f6188b846506', '42b5d7637f053404', '42ef59e3d08070f4', '4311f6c0035efab7', '432337aeb13ec3d1', '43259398a2b68a62', '435cfd19170dfb2c', '436a675af0073314', '43b90fd5628363ac', '43c695923f21b29c', '43e578f33fc9fe33', '43e68fbaea6a332c', '4407073bf04f2911', '4531010bbfdfb990', '458051727f6f60b5', '4642b8dade65b547', '4643a805bc44c15e', '4654ae3c486a6e47', '4661e7e2c712f2bc', '468f1faaced392d6', '46b94b260d1eaf42', '46f2af1337ce16f0', '46f7c5d92b3952a4', '46faca1a4b3610d7', '47800be3a12f09ec', '48057c7a9f1b6e3a', '488eb04ed6d0678a', '48cf943892bfb0e8', '48da622f95040d1e', '492f98397b199fd4', '493949471021aa14', '494633c7b3370261', '495b067ce7b117cc', '4960152eddfe1be9', '49e89f8658d7f0b8', '49fa77a68c5e2530', '4aec30b38e83148b', '4b5cd05ed2bf311a', '4b7ff06172b433d0', '4b899c67d3f72b0a', '4be2df647f022815', '4d05ba0a11b0b2be', '4d08cecd920db010', '4d1c38debb846c06', '4d502886971dd0bc', '4db0da0300a3740c', '4dd3720180839f04', '4ea4a32a108e91ca', '4ea8ae5fdd568a60', '4ec15d2a89cbc38e', '4f1680a2ed2e2ab6', '4f1a5d9369d4baee', '4f8a835155cdf1ba', '4f9022705ba48260', '4fb8264ba4105670', '4fbae547cff0abe9', '4fca5ce6fd6d5644', '5042914dfd17c993', '50e3c2c9a1d1ef00', '512202c24b5fe101', '512f368510af1d1e', '519e5ac6f4835c5a', '51dd5499384144ef', '51df39dea3372a7d', '51e88e5282f941de', '521c482fc1b61285', '5233b70cf65a682e', '52707a16abc3af00', '529ab16f75c2d206', '529f8606b618269b', '52ba8d96dc3ece88', '5325786949a90dee', '53a86b4aa90329ce', '53b5738f517c57c0', '53f6f826802a4221', '54250d97feb8e459', '544910314f36ab03', '545bdccca92ad23e', '54934f7e1aedfaac', '54a269d925e04327', '54a4f546e59ad219', '54c3edd49a3e535b', '554391f0e09c742c', '5544a75afdb5d91e', '55bbe2419e8e6faa', '55c081b2bdc3f6bd', '55c1af51f3d09fb3', '55cc62dd04262702', '55d86028248a6ad9', '55e3fb89bd465a67', '566a0b664474c764', '569d132a6daa63c2', '56c976b5ab07dfc3', '56cef40ad47b3d8b', '579e37ff272ce744', '57bfcca3a55c1de7', '582d72d1bb020c58', '5866e7455810ad0f', '5869537decf6fc3a', '5873d52de692f731', '5875a5173a6cc6c6', '588202aa52e8bba3', '58c78397e2baefff', '58f8c00b9ec76267', '5926e03f99959009', '5937666f6009852f', '5967b69255076d80', '59b72bcff2c7ebbd', '59bec0e6d0ec43fe', '59e29a6c6706d157', '5a173a18ad38c282', '5a27b3e5885cc511', '5aa1f803f094cc54', '5aa415a6e2cc9939', '5ab42ee105ded81f', '5af1bd5fd010ee69', '5b5eaa8b9498349f', '5be35e6edea1cbdd', '5d041c106463674c', '5d0b2140489bb52b', '5d3bf16fb7b91bea', '5d4363281bbac74b', '5d45663abb7a7884', '5d60c8cf64dc962d', '5e64b4d67a895d41', '5e6b3ee931f10e44', '5e8b0c30842d94de', '5ea289714c41216a', '5ef7da10fd19c38e', '5f110ec3022b3747', '5f408dd86a00c55d', '5f8a5bad7212cf4e', '5fb2b51793453d96', '5fe9f22e8f84739c', '60402ec9f295b5b3', '606a3cd24dfcb3f2', '60e819f48eb5443b', '6103607d9b7fb300', '6130cbacdb70efb9', '613a969c71f65be8', '6169448b8304521c', '618d2b06d44f269c', '621989719561c18d', '624257cd5959f4e0', '62a81f63f11d18e6', '6398e2a32e3b0919', '63c412fd8b09be52', '63cb0684e3684d1b', '6522f7f871e3272b', '653374f76b91290d', '653989c0d34463cf', '65de44c891081f01', '6601a2a1c5c29d80', '6609aba4a22df501', '6621ec65a485beb3', '66220222a6c3cca0', '66364eea7ec0b820', '6676176459a2edbd', '66b426e761c30842', '66f89b4860c660dc', '6734eb1da97d1df4', '67ffef4714728c33', '6807caf5582d2488', '6810224bc1b5a7de', '686bdc4ed95516cd', '68901e5a225cf73b', '69245e05ed6e2158', '6933a777f4bd879d', '6937a167e2d208c3', '698abce1dce9484f', '69d4c99c22d429e7', '6a21933080dbb320', '6ae90b2c7114883f', '6b5b4528ae4921d8', '6bc09e906cdfd874', '6c1ee0a2b8c1567f', '6c5e2e44d6143bd9', '6c6a07beeb38ea1a', '6c7cce4fa5db4bce', '6c94b902d1ecfa60', '6d10b4ed7123cebe', '6d40a9e37c4b479b', '6dee62cd67c89d93', '6e36176ee0640100', '6e7078d1ca94235b', '6e7324ce1833c6a4', '6f63ea40b937e393', '6f8dc49a01876dfa', '6fa40a09d9bdf9ed', '6fb8246ba20cec09', '6fdc89c36c3ff996', '6ff29619650b845e', '6ff74d3e7fd714e7', '703e22567af6b29f', '70404e149fb71582', '7063e9a329881177', '70753d31a7383b42', '70acae248e08582e', '70d43bc9cdfdbda6', '70d561fce7640b85', '70e4f63654b3d4c2', '70e743db7cb97048', '7136a9d4c1e242f4', '7160c13ac703632a', '71758b2fc957090d', '71d79c85a6bd156d', '7201fba52451a151', '7222ff2f6154e6a2', '72cd78d35d9ba204', '7358df1e9802fd50', '73614059e35074a5', '736620183dd4830b', '73ba2cdf245c3ccb', '744c648891db37b9', '751c09beda320f19', '7531fec282436fd3', '7593d789b6dc4836', '76a9e86693ee322e', '76c0f1f0521a504f', '77035144302b7bce', '7715428810cc7be8', '77180a1aea541e0d', '772bb29110152a8b', '77335ce55a85b505', '7752bde7f43e46a6', '777889a5e9787442', '77837058f53c5593', '7785bb1a3e46246e', '77992a88ddb07da6', '77a71f068c98b9eb', '77b826b1e0727615', '78436d380568f948', '7869aa033aa8beeb', '78b9e42025af99b8', '78d2b36ed33fef95', '78db929f90a57105', '78dc635701c207df', '7a7a94db31b92039', '7a86db13c55f70e8', '7aa4f69a36ef3dbe', '7abe02c363ad01c8', '7ad6c36df13dedda', '7b153eda53aaa527', '7b5b21122fd9e812', '7b73426e703270d6', '7b8a51d2b35a90e7', '7babb957e31080e7', '7c3e0148cf3fd044', '7c929b0e97d20edf', '7c94c8c901310f2d', '7cbf35f61989b278', '7ce28c559604a6fc', '7d11cb044071afe1', '7d23928b3658dc65', '7d493d554498fcb3', '7d5d4c25014db494', '7d7f0d538cfd947d', '7da6ccc91bd89b21', '7db1887b191f1801', '7dd6694d81686a3c', '7e118e9e8c50ec45', '7e62f8a6efeeeee0', '7ef16f85a58a1bbc', '7f0a9629520af956', '7f1afe0a76267550', '7f6cad49f6fed9e4', '7fbc382e262d627e', '7fede1e38f5decc8', '80b4cc85b2534c3e', '80b660f44863325c', '80dd8980c5ee9345', '80e51e3a2bed57b9', '8101053e016867f5', '813a109c0ebb6b38', '81525d4bf0999ca1', '815309ebb9d77a66', '816131778848f17b', '81bbe8e797aa749b', '81cb6be4eed8a2ab', '81ec281e637d2568', '8234a5b6151a3faa', '826181cc8ecf1847', '82bf63e273f24c80', '82fc03616b21fc3a', '83405e14e36dc6d3', '837d8b82e47d8738', '837f4d8d8a81dcbe', '83a1cd6503e7ca23', '83d5081ef53889a6', '83e74cb276724a38', '83f03cf4b67dc3af', '847492b7ad649791', '84ca9b238e3a8e05', '8513d87c36b5406d', '858711b3e25af9e7', '85990f17750158bd', '85d1b6caa91199fc', '85f9d89d4513ab5b', '85fc003da0500e02', '860c1635fc572622', '8683421ae4afeaff', '872cc255e2845c14', '87a7a2f9b61148a8', '87becde0ea6ac2da', '88052a1a90a9f5b1', '8831d98cfa430383', '883cc3de9219c8f5', '8849a4fefda2781f', '885f3eb1b0baa726', '886e1727b336792b', '88c3c3567b0a83c7', '8971a29e24c19915', '89a6498a52581338', '89e9649695f5edba', '89fd8be1ee2a5c90', '8a8d25a30f5f5df9', '8ab17475e9817095', '8add0fee9a051718', '8af87bef52cb325c', '8b80f642d11a2be5', '8bcb36ec9a5db1ce', '8bceac2e119baa26', '8befc7539a08b258', '8bf46a030d063410', '8c16f224f34ebbb3', '8c571c06ca00f6cf', '8c71985585cb4e4e', '8c7ab6aecff645b8', '8ca06991de108f4f', '8d170d64e04cd211', '8d4cf3aa1f3d8161', '8d6857f6ddcaf2c2', '8dcd678d101a1bfc', '8e0895b6bf99ee15', '8e2eaf78e8d53a86', '8e2ec62f6eb39027', '8e50394ecee84f2c', '8e5196cde9c308d7', '8e556e781347b9a8', '8e73a6ea829b4434', '8e79f49ac60fa1b0', '8e8113802aa711f8', '8ea65ab54414ce50', '8ec58c3ebdbc9405', '8eda84351a129e6c', '8f409ad167f1f935', '8f8adb5431b253c0', '8fe47b7f99c94f12', '903c4486af478e16', '90558b473d1ce06d', '9055e760069b79b8', '9063c5e137ce50f4', '90b98b55285f0613', '90c25b0eeb96f0cb', '90ffa485f2234301', '91c20931081cf97a', '91d3825a8b59643d', '927684d31b4468b9', '92d1533db0a94bc1', '930188a0ee172238', '93201b085f5c8244', '9331e396ccac4f41', '93359cf49177f6a4', '934449d4a461d48e', '9355fb7472d763d0', '93d4e933552f67c4', '93eae5c000e427cc', '93ef8ab5b8aa1ccc', '9426fdadbdab83e0', '942f5ae587a8250c', '94ba1ca5a8b5177d', '94f3ec9695510ba0', '951e5450c340f715', '95a658c50eb7c3d7', '95c633f9a5bd5723', '964197a4078087e2', '96c85e171074f3ea', '96cfd230e4d59b9d', '96def91e30ca9ee5', '96f02d7aeefacc6d', '9727e8e7d7f45fbc', '9787acd3f3f94bfa', '97a4780599e6a705', '97cc36167e31da26', '985c0cf2e7109243', '986c0cb9b1e9891b', '98affbe95f97e803', '98f74e4c6f54a534', '98fd0eaa629527e2', '99027bb9391fcbf6', '99b0a1449ba94771', '99b4183340d82807', '99f1a21713ee4aca', '9a419dc01e7f548c', '9a5e7538cfb53f08', '9ae68ec359566800', '9b13bb9da393526f', '9b9468b3db145f29', '9bc57b994e2408f4', '9bc9b56c0d71ddbe', '9bcf8235755b2490', '9bfebb31ae067111', '9c50b94ee2429675', '9c784283d4fb00e1', '9d7bdf01b05ba518', '9e2e28cf35f7fa14', '9e707efa13dcb644', '9f5c2507367bb89e', '9fbe4d789dbfb962', '9fde17f36a71e587', '9feafddacebf43dc', 'a00b3ae24e8714cf', 'a07f79f58f007add', 'a083487eae16f4ba', 'a08cfe2fd46a96d2', 'a0ac6f9cd7f7fa6e', 'a0b79db0ed8b43a8', 'a0f7927bb013223c', 'a158d58b925c8d43', 'a1acea295c256da2', 'a1ada388c53ef044', 'a1b0c2246f256ccd', 'a200a09d21811359', 'a21fa38c3c07b656', 'a22e81cbefa07563', 'a2dee47062e86899', 'a3672fe82b0a8c4d', 'a3b3e6899ece1171', 'a3c23bb97f3b9627', 'a45fdd02a9c2b259', 'a46dee5e858b9cbb', 'a4af368ce2c36287', 'a4c2bb33d3f97ebd', 'a502a7e01152e968', 'a510d28b44d30e5e', 'a52735367a22c614', 'a5567a62ea2e9966', 'a58bbb46724308d8', 'a58fa1a672b480e7', 'a5a95a9a5b414102', 'a6446f3ce8652842', 'a66fcedb47a1d672', 'a6971e404356b2f9', 'a6bca181f4b5be25', 'a70896b7f69176d2', 'a75f4f56b6b1ed9a', 'a7961b6f02579c88', 'a7c0ea845a6cf20b', 'a7c3c0f6998dddd7', 'a7c99db96d7a3ce7', 'a7db09fd22e70d4e', 'a83f75d8d3bbe3f1', 'a843f6e014e52bc6', 'a84cca496613e27a', 'a894d65b31d2f097', 'a8facb44b832496d', 'a9133af47ee50f07', 'a95fc107defa862b', 'a989026e5cc90a74', 'a9b639ae69f0df61', 'a9b988893cf58f53', 'a9d9504d1d448877', 'a9e606e3da938d85', 'aa0def23b14ae5dd', 'aa57054bf9d67946', 'aa63a7db9ee5c86a', 'aa97e97e9f5df493', 'aada18fbb7350e74', 'ab206d92b5b8e921', 'ab7a4cc270d4ad66', 'ab9f8f78ebc83db4', 'abbda6b12b631308', 'abc4bfc3200beb50', 'abefcbd804012624', 'abf52fd33da60877', 'ac41d20c0c059c3e', 'ac7b90074dbb4aaf', 'ac8893306c151b3e', 'acae0456dcea987b', 'ad3a37b1059d6a0b', 'ad5dfddae865f7bc', 'ad6bb8485c77a7b8', 'ad86ca702c76e96c', 'ada191a3b615f1f3', 'adf2badf47dc9c00', 'ae1fb7ff29a36234', 'ae51c2418351f228', 'ae8fc6dbc9d3e04c', 'aec3abe4afe2dffd', 'af216f394bd87f6c', 'af61d52004eeae4b', 'af7a0c9d5bc8293e', 'af8f2817acf2a63b', 'af94cddd2dcdb8a8', 'afd898dc0df71f9a', 'aff3ffbd9b70f06a', 'b05b64d540b3a5c1', 'b094f916a2ae9fa7', 'b0968f1bfb581452', 'b103361e544f4ae8', 'b12c2338e22b3f0e', 'b1c9d4211529af96', 'b1f5c09c67e90302', 'b1fbd7596d5ce44d', 'b24ee3e4e45d447e', 'b270cb90cdef4b3d', 'b2ab44f64bff47ca', 'b3965cc27f1fa515', 'b3e76ac3b1099da1', 'b41d89c96b096bf7', 'b41f4129bd537d89', 'b468fe3946d3dbfe', 'b585fced5f1d04d6', 'b5b3d0fc9dc19180', 'b5b46abefa7a8b25', 'b60fb442ffe5226b', 'b620f72929e10ee3', 'b6d76973fa2d47ec', 'b73ecc759ffcb37a', 'b78ab6f611aa4a2d', 'b7bb4d2c73739c65', 'b81e149f10968130', 'b839397e6390b824', 'b85b7e82dd39ddbc', 'b8a25517055bff02', 'b8e7848f627b033e', 'b8ebe433aba0743c', 'b8f2cf592ea80b7e', 'b8fc177d9b001da0', 'b90aafce0a5a2219', 'b92a7a62281ee71a', 'b93bdac634bbc89c', 'b93cb472d65ec1f6', 'b93e4e09f4aa4636', 'b9418ab02ef5390e', 'b98c62cf435eb965', 'b9cc7dfa184376b2', 'ba0a680bc2536bca', 'ba0f8741e6af52d9', 'ba13cdf83d0e0ac5', 'ba2bb92739584b82', 'ba37358c2c453e65', 'ba9805953c036541', 'bad20d51220e8b65', 'baf0d553142a3f7e', 'bb4143dc373c522d', 'bb4cb0634c97b0ff', 'bb7fa6788ecd1129', 'bb84981672aaa8c9', 'bba0d8d0522955d8', 'bba766c100a8eb9a', 'bbf9db66c9b9a66a', 'bc01449261465a60', 'bc0790dd5bf1676f', 'bc375d482f67a326', 'bc73a60a9f027d0b', 'bcba9aba61cbd3b6', 'bceef6008158ffb8', 'bd380f4a00b3d109', 'bd48505613a33e26', 'bd79217b2a8c11f8', 'bd79b57ea0e358f0', 'bdb392a903e8abb5', 'bdd0c18ed494c715', 'bdd6ec76104249b1', 'bddf6dbd2f3094bd', 'be22851fd197e061', 'bec7df7df54c71ba', 'bef8e02d63a1dff9', 'befa2dd7bc991f56', 'bf3ee1b09639d54e', 'bf8a9ec8ca53d4b1', 'bfbb4b2354017750', 'c009400a5f8c33a0', 'c03c918ee5ef3f7e', 'c03cdc6511054716', 'c061155be610270f', 'c0abccf89d1d1e6e', 'c0c48d56f3505c7a', 'c10976f8606f2819', 'c21296dbf734e05d', 'c22a7c009943b0ec', 'c22c358b3a0e3ed6', 'c256d419092db957', 'c27848068a0b2163', 'c2a16fcc6dfc7beb', 'c2d720f4cd846203', 'c317070f91838e5e', 'c37008cdd2b06cb4', 'c3947cbffe9932f0', 'c3b1e08231b82cf7', 'c3ba158b3438db76', 'c40583dafc9e8be5', 'c40a9fdc3d7c171c', 'c416f11ebadc2533', 'c427e655c1db65fd', 'c433c26a66be5383', 'c45487e94c753b92', 'c481d40ea53711eb', 'c4e48f779849e846', 'c4faf1093802809b', 'c5317be1aa6eebfb', 'c539ccd0c7489940', 'c5463992fded2498', 'c56b5bbe63760eb1', 'c59024f6bbb7c0ee', 'c590d33776e621a8', 'c683f85581b49317', 'c6c4bf82dee5892b', 'c6ef3382ff72c1f7', 'c6f6e0e451f6af59', 'c739d3f852273ba2', 'c75a2cc4189ac42b', 'c77da42535f8a96c', 'c788c88726a82085', 'c7c984cae9d608e8', 'c8821a0a644d0cc0', 'c8ee61e27dd3b01a', 'c916487af073337f', 'c91ef5496e6a0872', 'c92cb45b3ad183a3', 'c94c15c9da06056f', 'c99931c3353823d7', 'c99c609ab2fd69ab', 'c9d86c58a561c3a8', 'c9dd479392aced5f', 'c9e508de9ee17750', 'c9eb9aab80c14c25', 'ca0204420c2770df', 'ca4c7367ec0a122a', 'ca59d76595022254', 'ca6eea1fa11c1d39', 'caaa1d1ff6cc6911', 'cab7aa813c4934ce', 'cb5b22a5fe970dd5', 'cb7c05f84aa94cbc', 'cba195d9f0cb71bc', 'cbb4c65e112d4a6a', 'cbbd18210efcf097', 'cbcb265776c80ce5', 'cbd015c14d2359cc', 'cbe16175fab77702', 'cbf25c27ba7b880a', 'cc5513e333dc7ac8', 'cc60c536fc7f9125', 'ccf24b930d733f4b', 'cd5e90820df5c4c0', 'cde6ef8ceb129820', 'cdf227b784a0b8e7', 'ce172225795ed437', 'ce376e481579526f', 'ce593472652615ea', 'ce609b2c9fc07f16', 'ceac7984855a62b9', 'ceda7320cdd8a190', 'cefb8acd38f0c13d', 'cf21b65d67ef5753', 'cf81961bd71f5ce7', 'd006e060270c6ac9', 'd00c0744288ba3e4', 'd041b2e5184524fd', 'd04a8b64e8b2a0e2', 'd087a0dd9bac5da9', 'd09b9574ec6839c4', 'd0faa8a45a6346f3', 'd10cd51ca08b46a8', 'd18b1354af79deb8', 'd19bd53072f94bc0', 'd1a806af29c19bad', 'd1e95aef70b86659', 'd24d54b6d6155acc', 'd3254aba75b4bee9', 'd377803893d3942c', 'd3cb2407bdadca95', 'd401bd2bed8f51b6', 'd4253dd22c8b8a45', 'd47c5b0f926c631f', 'd4b17be05415ed2b', 'd4be29af46bf4cf6', 'd5192d23568c3816', 'd544e2de76423e3d', 'd5fc3ed740df1147', 'd613016eebc4fbef', 'd615f35590f16760', 'd639e27331794e9e', 'd6479fa57fdb1b08', 'd69a2da8dc8c6ad4', 'd6dc90fa8d96152f', 'd71bede290f4c4ed', 'd7733067efda67dd', 'd7cc5a8f0db430e4', 'd7cfceae201e9dff', 'd89d327ff7b87edd', 'd8a9345aa29e71c2', 'd8bc8f9921a31fd2', 'd8c5f89e2f567791', 'd8fdd65a92740db0', 'd8ff9ff9f49df25d', 'd907914008484717', 'd907a1121c915670', 'd94f69fc79b690ff', 'd95844c5407b8ed9', 'd98705fb8fbd0aca', 'da079fb5c0aa9230', 'da78286c9f2c1349', 'dad05fda7338562c', 'db6ac61e0eac7e7f', 'dc29cccdca4feaac', 'dc5e7c543879eca6', 'dc93724f3de12fe2', 'dcf253b028495262', 'dd04522f339cafa9', 'dd057efcecd9dee6', 'dd2265d17f6043e3', 'dd880f968dc367a6', 'ddabe83a69913cd5', 'ddd7a8499f0b3818', 'de1e0404f99cb194', 'de58e1bd5c3291ab', 'de7de2341ad5d5d5', 'dea391271f0d26e4', 'deb0c07321471744', 'df371e039d28780c', 'df3c8eaa9e52ed32', 'dfbfa56e49b83b53', 'dfe82fe886fdce96', 'dfecae28b4e2e803', 'e0168bca2e67cf8b', 'e058798c72077f31', 'e093923965b61451', 'e0a41a647e8f1831', 'e0ab372a47fa2aa3', 'e0bbb8fbca6601fb', 'e0f1bb5da0a788c1', 'e103995a8192872c', 'e12cbe82a21f8132', 'e176f6b4683d2e55', 'e20acb5d9c95f394', 'e2505148789aec0d', 'e25349ddd923b384', 'e26d8dcd98457411', 'e2bcb9db2f746ce0', 'e2c936aea5d04138', 'e2d32f889c2795c3', 'e2ffc821e65be05b', 'e3eb80f8aeec6d72', 'e3ff889d7e91a6e1', 'e46ddc7757b1b8e9', 'e5008b63b74ee288', 'e52fa9769cd21b3d', 'e57bc7eda88a31f7', 'e5b91b4b6f5c2b7a', 'e639eb2267231658', 'e69b2680e6aa14af', 'e6c4c413dff73a30', 'e7308e017d1e59c0', 'e741226f347a2778', 'e781d2b4f6fb9ae4', 'e78bf2d028d80c1a', 'e80de81f95159c05', 'e848c6e661f3be3e', 'e8737008e49bb409', 'e9283491144a3bca', 'e9464dfd3e975a02', 'e99ee12c31507c5b', 'e9f5e161b79c12cb', 'ea7774395f6e8a62', 'ea7ffac68cd7eb45', 'eae2b5fd370f7016', 'eb3deff16b2ffce7', 'ebb6c87e2a1aee1c', 'ebdb14592a42d7fa', 'ecaed3b725e79b2b', 'ed5dfb53b7fa8310', 'ed6546adeca0f8cf', 'eda4ab08de05a3bb', 'ee2cea563304a8bb', 'eeddb40a62e4524b', 'efb62cf82551f935', 'f050f0277e91106b', 'f05b39d31c566c16', 'f077bcc4b169598d', 'f077d69bf482f3d6', 'f094b5935819453c', 'f0c3a64ad4ade2b5', 'f0c40ddfa5da97e5', 'f0ec6caaf4e86645', 'f12de81b2b2a7d79', 'f14ed49a4fb01220', 'f1eec21d8ff6acc3', 'f204a184f03d1db4', 'f23fee97c1700441', 'f240b2f8d0a85b7d', 'f25986db55e47747', 'f279631ec1f7efb0', 'f2cd4bc46f085cb3', 'f2f936abab0f51b1', 'f2fb1915eb1560f7', 'f32f8faa7401f7fd', 'f34e1ecccab3e7e4', 'f35cedc18522fcb1', 'f35ec613a1356f6a', 'f39855130524f112', 'f3cf7cd2d5a62b07', 'f4530228f6adb8fc', 'f486d209047d7669', 'f497a071380cf4e2', 'f4e66f8f7cafc303', 'f4e68ac7a5ca7ed4', 'f4ec05dd75321474', 'f54c65bcc62772b3', 'f55a7688434feab3', 'f5a0639af244fc92', 'f5a5238fb0381d64', 'f5e216a328e7148d', 'f6288ee301c1e84e', 'f64e30519180075b', 'f659516ecf91615c', 'f69c9ad51ecaf093', 'f6dff8eb5aa697f5', 'f6e8d0f0d49e9d6b', 'f710c4f8a50ad571', 'f788641d693e328e', 'f7c53496aa572545', 'f7d121cf5e2299b6', 'f823070e09001b8f', 'f826e7e3cda394ba', 'f85e7fa48395978d', 'f85e8096eb4bfb91', 'f8bdbdc8bcae4c8d', 'f8d25a7961118d85', 'f91eb71f28c06273', 'f9b6906be59926b9', 'f9dc4d661deb57f2', 'fa0af9ea04a0ee43', 'fa433b3c08bbcd90', 'fa6988b3014d4537', 'fad6b78b1fcb3daf', 'faf9e6abbbb0e8f3', 'fb60044be4b70e8b', 'fb779eb31339f9ec', 'fb938f65e01414cc', 'fc7333c720887bc7', 'fcbdd0e4c7ef7c31', 'fce318039a0cbac4', 'fd6efa42cad89d67', 'fe420c0a38418808', 'fe639b0c116c0f7f', 'fea3c448ab9e14f6', 'fee0d7126c9be0d6', 'ff3b403412e506e8', 'ffb123584152b95e']\n"
          ]
        }
      ],
      "source": [
        "# Create list of images in present directory\n",
        "ImageID_list = [] # Create empty list\n",
        "for entry in os.scandir('./Student'): # Yields DirEntry objects that include file type\n",
        "    if entry.is_file(): # Checks if entry is a file\n",
        "        ImageID_list.append(os.path.splitext(entry.name)[0]) # Splits the path name into a pair root and ext.\n",
        "print(ImageID_list) # Print list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFwyupB2dSYX",
        "outputId": "7a6dea81-0ca4-4675-a69e-53cf859c3e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             ImageID                                        LabelName_y  \\\n",
            "0   00003e2837c7b728  /m/011_dp /m/011l78 /m/011_my /m/011q46kg /m/0...   \n",
            "1   00244fb73e839998  /m/014cnc /m/014jg3 /m/016c3c /m/017ftj /m/019...   \n",
            "2   002730ebd4e115d1  /m/014cnc /m/0177t1 /m/018w1s /m/0191f8 /m/01c...   \n",
            "3   002b526d760fcd22  /m/011l78 /m/014cnc /m/014jg3 /m/015ll /m/018t...   \n",
            "4   003185aacd1c0be9  /m/014cnc /m/0191f8 /m/019nj4 /m/01c648 /m/01d...   \n",
            "5   00c1b2e23be6d099  /m/014cnc /m/015ll /m/016c3c /m/019nj4 /m/01bl...   \n",
            "6   00e57d3e8d9b9a55  /m/011l78 /m/014cnc /m/019nj4 /m/01b4q9 /m/01g...   \n",
            "7   011d79b4b8aefdd9  /m/014cnc /m/019nj4 /m/01c648 /m/01gd1c /m/01l...   \n",
            "8   019a1d9e8ab79a70  /m/014cnc /m/019nj4 /m/01c648 /m/01g317 /m/01g...   \n",
            "9   02c174c069aceed8  /m/014cnc /m/018w1s /m/0191f8 /m/01c648 /m/01l...   \n",
            "10  032a24cd420692e1  /m/014cnc /m/019nj4 /m/01c648 /m/01g317 /m/01m...   \n",
            "11  03aea9956c16f5d1  /m/014cnc /m/01c648 /m/01g317 /m/01gd1c /m/01j...   \n",
            "12  03b51991527dae7c  /m/014cnc /m/019nj4 /m/01fcnx /m/01g317 /m/03b...   \n",
            "13  048c8e1570f3a421  /m/014cnc /m/019nj4 /m/01bl7v /m/01g317 /m/025...   \n",
            "14  058b43c67f38b380  /m/014cnc /m/0191f8 /m/01c648 /m/01g317 /m/01g...   \n",
            "15  068c6a6381647e65  /m/014cnc /m/019nj4 /m/01bl7v /m/01g317 /m/03b...   \n",
            "16  0fc938f92aca76f9  /m/014cnc /m/019nj4 /m/01c648 /m/01g317 /m/03b...   \n",
            "17  12da508dc84a94ee  /m/014cnc /m/019nj4 /m/01g317 /m/03bt1vf /m/04...   \n",
            "18  1311dcc534e4e1cf  /m/014cnc /m/019nj4 /m/01bl7v /m/01g317 /m/01r...   \n",
            "19  137b07a5b73fc40b  /m/014cnc /m/016c3c /m/01g317 /m/01ysy9 /m/046...   \n",
            "20  13f78892f7cc55fc  /m/014cnc /m/019nj4 /m/01g317 /m/03_055 /m/03b...   \n",
            "21  17c3472ab84f0ad5  /m/014cnc /m/019nj4 /m/01g317 /m/01m3v /m/03bt...   \n",
            "22  1924e3a2cccd3e54  /m/014cnc /m/019nj4 /m/01g317 /m/025tjcb /m/03...   \n",
            "23  1a5cf3875e1eccb7                      /m/014cnc /m/01g317 /m/02rdsp   \n",
            "24  1f567a46011697a4  /m/014cnc /m/014sv8 /m/019nj4 /m/01c648 /m/01g...   \n",
            "25  2358b33076d621e9   /m/014cnc /m/019nj4 /m/01g317 /m/05r655 /m/09j2d   \n",
            "26  512f368510af1d1e              /m/014cnc /m/01g317 /m/09j2d /m/0dzct   \n",
            "27  813a109c0ebb6b38  /m/014cnc /m/01c648 /m/01g317 /m/01m3v /m/03bt...   \n",
            "28  93359cf49177f6a4             /m/014cnc /m/01g317 /m/05r655 /m/09j2d   \n",
            "29  b41f4129bd537d89  /m/014cnc /m/01c648 /m/01g317 /m/01m3v /m/01y9...   \n",
            "\n",
            "                                        Description_y  \n",
            "0   Membranophone Team Idiophone Container Cucurbi...  \n",
            "1   Student Training Graduation Sunglasses Learnin...  \n",
            "2   Student Plaid Public library Learning Laptop T...  \n",
            "3   Team Student Training Board game Service Learn...  \n",
            "4   Student Learning Smile Laptop Teacher Secretar...  \n",
            "5   Student Board game Graduation Smile Boy Laptop...  \n",
            "6   Team Student Smile Social group Person Private...  \n",
            "7   Student Smile Laptop Test Chess Call centre St...  \n",
            "8   Student Smile Laptop Person Test Tutor Magistr...  \n",
            "9   Student Public library Learning Laptop Librari...  \n",
            "10  Student Smile Laptop Person Computer Call cent...  \n",
            "11  Student Laptop Person Test Spelling bee Call c...  \n",
            "12  Student Smile Secretary Person Woman Lawyer Ma...  \n",
            "13  Student Smile Boy Person Secondary school Man ...  \n",
            "14  Student Learning Laptop Person Test Computer P...  \n",
            "15  Student Smile Boy Person Woman Man Girl Clothi...  \n",
            "16  Student Smile Laptop Person Woman Man Girl Clo...  \n",
            "17  Student Smile Person Woman Man Girl Clothing F...  \n",
            "18  Student Smile Boy Person Private school Second...  \n",
            "19  Student Graduation Person Diploma Fashion acce...  \n",
            "20  Student Smile Person Cornrows Woman Girl Clothing  \n",
            "21  Student Smile Person Computer Woman Girl Cloth...  \n",
            "22  Student Smile Person Secondary school Woman Ma...  \n",
            "23                     Student Person Office supplies  \n",
            "24  Student Human eye Smile Laptop Person Computer...  \n",
            "25                 Student Smile Person Girl Clothing  \n",
            "26                 Student Person Clothing Human face  \n",
            "27  Student Laptop Person Computer Woman Man Girl ...  \n",
            "28                       Student Person Girl Clothing  \n",
            "29  Student Laptop Person Computer Desk Woman Tabl...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-95-f10909d0220c>:4: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  train_Student = ImageID_subset.groupby( # Groups using a mapper or by a series columns.\n"
          ]
        }
      ],
      "source": [
        "# Subset the data based on ImageID_list\n",
        "ImageID_subset = train_Student.loc[train_Student['ImageID'].isin(ImageID_list)] # Whethers each element in the DataFrame is contained in values.\n",
        "\n",
        "train_Student = ImageID_subset.groupby( # Groups using a mapper or by a series columns.\n",
        "    'ImageID')['LabelName_y','Description_y'].agg( # Aggregates using an operations over the specified axis\n",
        "    ' '.join).reset_index() # Joins all items in a tuple into a string\n",
        "\n",
        "train_Student.to_csv('train_Student.csv', # Writes object to a csv file\n",
        "                     encoding='utf-8', # Encodes as UTF-8 bytes\n",
        "                     index=False) # Writes names\n",
        "\n",
        "print(ImageID_subset) # Prints DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2s7eZS_dSYX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g26NYFlddSYX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OagP6oLmdmz"
      },
      "source": [
        "__Step 12:__ Perform another EDA on your ```train_[fill in class description value]``` table. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49S5G2ivmfJ4"
      },
      "outputs": [],
      "source": [
        "#Add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4IAef-qqhCp"
      },
      "source": [
        "## __Learning Objective 2: Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset (Part 1, Pre-processing)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6smHYtWdUy2w"
      },
      "source": [
        "### __Mitigate bias__ \n",
        "\n",
        "Unfortunately, most of the current fairness enhancing techniques only apply to tabular data. Our project is focused on image data. Thus, we have to be a bit creative here. One way to mitigate bias in the pre-processing stage is by applying the 'fairness through unawareness' method, which basically means that you remove as much 'sensitive' data as possible. In our wedding example, that would encompass the rows of data where the ```Description_x``` value equals 'Ivory'. \n",
        "\n",
        "__Step 1:__ Apply the 'fairness through unawareness' method to your Open Images V4 dataset (meta/image data!). Elaborate on your approach. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLLfYRd_vZtU"
      },
      "outputs": [],
      "source": [
        "#Add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCqkWRjrxeOJ"
      },
      "source": [
        "Add your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEcLaNLovbRx"
      },
      "source": [
        "Sometimes, you do not want to remove any data instances. For example, when you already have a relatively small dataset. Remember neural networks require a vast amount of training examples!  \n",
        "\n",
        "Another method to mitigate bias, which does not reduce the size of your dataset, is 'fairness through awareness'. In our wedding example, this would mean that we need to include images that depict a wider range of wedding garments. To make our dataset more inclusive (and thus representative), we could for example scrape the web for images of Lebsa lfasiya dresses, and add those to your dataset.\n",
        "\n",
        "__Step 2:__ Apply the 'fairness through awareness' method (i.e. add new instances) to your Open Images V4 dataset (meta/image data!). Elaborate on your approach.\n",
        "\n",
        "For a web scraping tutorial in Python, run the code below: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "mZ76S-a61SuT",
        "outputId": "e36af0af-8f3a-4704-d899-44f51185e397"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lisae\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/stIxEKR7o-c\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/stIxEKR7o-c\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JnYghlF1wUc"
      },
      "outputs": [],
      "source": [
        "#Add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPQXWqg11RI"
      },
      "source": [
        "In the unfortunate situation, when there is no additional data available. You can apply data augmentation techniques to your dataset to improve the performance of your ML model (i.e. reduce overfitting!):\n",
        "\n",
        "> Data Augmentation is a technique that can be used to artificially expand the size of a training set by creating modified data from the existing one ([Source](https://neptune.ai/blog/data-augmentation-in-python)).\n",
        "\n",
        "Keep in mind, the goal of these transformations is to make your dataset either more balanced and/or representative.\n",
        "\n",
        "__Step 3:__ Identify images that depict/represent marginalized groups (e.g. non-ivory/white wedding dresses, LGBTIQA+ couples) in the Open Images V4 dataset, and transfer them to a new directory. See code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Cavx0RD6Uc4"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_dir = #Insert image source directory\n",
        "dst_dir = #Insert image destination directory\n",
        "image_names = #Insert image name (without their extension, e.g. .jpg)\n",
        "for image_name in image_names:\n",
        "    shutil.copy(os.path.join(src_dir, image_name+'.jpg'), dst_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-dtPKWDyyK"
      },
      "source": [
        "__Step 4:__ Apply data augmentations techniques to your subset, and visualize your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YLBc-IzCrkp"
      },
      "outputs": [],
      "source": [
        "#Add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ2fAFnTRWL0"
      },
      "source": [
        "__Step 5:__ Add the augmented images to your train set/directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jDtOss9RR3D"
      },
      "source": [
        "## __Learning Objective 2: Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset (Part 2, In-processing)__\n",
        "\n",
        "\n",
        "Say we want to train a binary classifier that can predict if an image depicts a wedding or not. As with the Open Images V4 dataset, our 'imaginary' dataset mostly contains images of traditional American (i.e. Western) weddings. Images depicting traditional Morrocan weddings are underrepresented in the data. See data distribution below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHmw4iW7wKKE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from textwrap import wrap\n",
        "\n",
        "classes = ['X_label: Traditional American Wedding', 'X_label: Traditional Marrocan Wedding']\n",
        "classes = [ '\\n'.join(wrap(c, 20)) for c in classes ]\n",
        "y_pos = np.arange(len(classes))\n",
        "count = [10000,1000]\n",
        "\n",
        "plt.bar(y_pos, count, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, classes)\n",
        "plt.ylabel('Number of X_labels')\n",
        "plt.title('Y_label = Wedding')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBfYMA2P9s_x"
      },
      "source": [
        "With correct bias initialization during training you can mitigate (some of the) bias introduced by the unbalanced dataset: By putting emphasis on the minority class the model does not spend the first few epochs just learning that the minority examples are unlikely (i.e. learning the bias). In Keras initialize the bias with the function [```bias_initializer```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). \n",
        "\n",
        "In the case of the log loss metric, the baseline of dumb, by-chance prediction is 0.693 for a balanced dataset. This number is obtained by predicting the prevalance or p, and value it at p = 0.5 for any class of the binary problem:\n",
        "\n",
        "Log Loss = -log(p)\n",
        "<br>p = (1 / N)\n",
        "\n",
        "N|2|3|5|7|10|15|20|30|50\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "**Log Loss**|0.69|1.1|1.61|1.95|2.3|2.71|3|3.4|3.91\n",
        "\n",
        "Code examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6EVNi-4xd05"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "prev_class_1 = 0.5\n",
        "prev_class_2 = 0.5\n",
        "dumb_baseline = 0.5\n",
        "\n",
        "log_loss = - prev_class_1 * (math.log(dumb_baseline)) - prev_class_2 * (math.log(dumb_baseline))\n",
        "\n",
        "#OR\n",
        "\n",
        "#N = 2\n",
        "\n",
        "#log_loss = -(math.log(1 / N))\n",
        "\n",
        "print(log_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV1n1wj-zTQL"
      },
      "source": [
        "The baseline of dumb, by-chance prediction value gets increasingly smaller when the data becomes more and more unbalanced. For example, a log loss value of 0.5 with p = 0.1 (i.e. prevalance of traditional Marrocan wedding labels) indicates that the model is performing poorly.\n",
        "\n",
        "p|1|2|3|5|10|20|30|40|50|60|70|90|95|97|98|99\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "**Log Loss**|0.06|0.1|0.13|0.2|0.33|0.5|0.61|0.67|0.69|0.67|0.61|0.33|0.2|0.13|0.1|0.06\n",
        "\n",
        "\n",
        "A correct dumb baseline for this particular dataset would be around:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzWT5XRauVOV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "prev_minority_class = 0.1\n",
        "prev_majority_class = 0.9\n",
        "\n",
        "log_loss = - prev_minority_class * (math.log(prev_minority_class)) - prev_majority_class * (math.log(prev_majority_class))\n",
        "\n",
        "print(log_loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDm8HW7uEzy5"
      },
      "source": [
        "If you set the initial bias properly (in the last dense layer of your network), the model is likely to provide more reasonable initial guesses. Code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPAPg0z0PKuW"
      },
      "outputs": [],
      "source": [
        "correct_bias = np.log([prev_minority_class/prev_majority_class]) #Count of X_labels: Traditional American wedding, and traditional Marrocan wedding.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid', bias_initializer=correct_bias))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGhM_gdqPTSG"
      },
      "source": [
        "In addition, try to complement your analysis with metrics that focus on the minority classes, such as recall.\n",
        "\n",
        "For more information on bias initialization in Keras: [Classification on imbalanced data](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data). \n",
        "\n",
        "__Step 6:__ Add a custom bias_initializer to your Keras model. Retrain your model, and evaluate its output. What was the effect of the custom bias_initializer? Elaborate on your answer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQT0aq9-Oswb"
      },
      "outputs": [],
      "source": [
        "#Add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD7BSF6DTiRI"
      },
      "source": [
        "## __Learning Objective 2: Propose, and apply appropriate fairness metrics, and debiasing techniques to the Open Images V4 dataset (Part 3, Post-processing)__\n",
        "\n",
        "Although, the fairness libraries (e.g. aif360) currently do not support image data. You can calculate many of the metrics by hand. For instructions see, [Fairness: Evaluating for Bias](\n",
        "https://developers.google.com/machine-learning/crash-course/fairness/evaluating-for-bias).\n",
        "\n",
        "\n",
        "__Step 8:__ Apply post-processing fairness enhancing metrics to the output of your ML model (e.g. equality of opportunity). Evaluate your result. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tlxBNHrQIeP"
      },
      "outputs": [],
      "source": [
        "#Add your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQRKihBF7cyN"
      },
      "source": [
        "## __The End__\n",
        "\n",
        "![Alt Text](https://media0.giphy.com/media/27Y1W0GCKQtDq/giphy.gif) \n",
        "\n",
        "__Title - Responsible AI: Open Images V4__\n",
        "<br> __Author - Irene van Blerck__\n",
        "<br> __Created On - 11 Januari 2021__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BISfTA6tdSYa"
      },
      "outputs": [],
      "source": [
        "# Try 1 of getting random image from file\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "print(os.listdir('./Student'))\n",
        "\n",
        "multipleImages = glob('../Student/**')\n",
        "def plotImages2():\n",
        "    r = random.sample(multipleImages,2)\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.subplot(331)\n",
        "    plt.imshow(cv2.imread(r[0])); plt.axis('off')\n",
        "    plt.subplot(332)\n",
        "    plt.imshow(cv2.imread(r[1])); plt.axis('off')\n",
        "    plt.subplot(333)\n",
        "plotImages2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezdyuqapdSYa"
      },
      "outputs": [],
      "source": [
        "# Try 1 of getting row data when cell contains value\n",
        "dff = ImageID_subset[ImageID_subset['ImageID'].str.contains('613a969c71f65be8', 'f12de81b2b2a7d79', '003185aacd1c0be9', '32ef8d7ac81981a6', '621989719561c18d')]\n",
        "print(dff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efC8drFodSYa"
      },
      "outputs": [],
      "source": [
        "# Try 2 of getting row data when cell contains value\n",
        "ImageID_subset.loc[ImageID_subset['ImageID'] == 'f12de81b2b2a7d79']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "openimages_assignment_student.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}